#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
driver_business_quickstats_mix_v2_optimized.py
---------------------------------
åŒºåŸŸ/æµå‘å½’ä¸€æ­£åˆ™+LLM+ç¼“å­˜ï¼Œè¯¦ç»†æ—¥å¿—ã€‚
"""

import os
import re
import sys
import json
from pathlib import Path
from dotenv import load_dotenv
import pandas as pd
import openai
from openpyxl import load_workbook
from openpyxl.worksheet.table import Table, TableStyleInfo
from openpyxl.utils import get_column_letter
import logging

# ===== æ—¥å¿—é…ç½® =====
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s %(levelname)s %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
    handlers=[logging.StreamHandler(sys.stderr)]
)

# ===== ç¯å¢ƒä¸Key =====
load_dotenv()
openai.api_key = os.getenv("OPENAI_API_KEY")
MODEL = "gpt-3.5-turbo"

# ===== è·¯å¾„é…ç½® =====
BASE_DIR = Path(r"E:\kabuda_data_analysis\å¸æœºä¸šåŠ¡ä¿¡æ¯åº“")
CSV = BASE_DIR / "å¸æœºä¸šåŠ¡.csv"
XLSX = BASE_DIR / "å¸æœºä¸šåŠ¡.xlsx"
XLSX_OUT = BASE_DIR / "å¸æœºä¸šåŠ¡_ç»Ÿè®¡åˆ†æ.xlsx"
CACHE_PATH = BASE_DIR / "area_normalize_cache.json"

# ===== æ•°æ®è¯»å– =====
if CSV.exists():
    df = pd.read_csv(CSV, dtype=str)
elif XLSX.exists():
    df = pd.read_excel(XLSX, sheet_name=0, dtype=str)
else:
    raise FileNotFoundError("æœªæ‰¾åˆ°å¸æœºä¸šåŠ¡.csvæˆ–å¸æœºä¸šåŠ¡.xlsx")
df = df.fillna("")
logging.info(f"è¯»å–æ•°æ®ï¼Œè¡Œæ•°: {len(df)}")

# ===== åŒºåŸŸå½’ä¸€ï¼šæ­£åˆ™/æ‰‹å·¥æ˜ å°„ + LLM + ç¼“å­˜ =====
AREA_MAP = {
    "Toronto": "å¤šä¼¦å¤š", "å¤šä¼¦å¤š": "å¤šä¼¦å¤š", "toronto": "å¤šä¼¦å¤š",
    "North York": "åŒ—çº¦å…‹", "åŒ—çº¦å…‹": "åŒ—çº¦å…‹", "northyork": "åŒ—çº¦å…‹",
    "Scarborough": "å£«å˜‰å ¡", "å£«å˜‰å ¡": "å£«å˜‰å ¡", "scarborough": "å£«å˜‰å ¡",
    "Markham": "ä¸‡é”¦", "ä¸‡é”¦": "ä¸‡é”¦", "markham": "ä¸‡é”¦",
    "Richmond Hill": "åˆ—æ²»æ–‡å±±", "åˆ—æ²»æ–‡å±±": "åˆ—æ²»æ–‡å±±",
    "Mississauga": "å¯†è¥¿æ²™åŠ ", "å¯†è¥¿æ²™åŠ ": "å¯†è¥¿æ²™åŠ ",
    "Etobicoke": "æ€¡é™¶ç¢§è°·", "çš®å°”é€Š": "çš®å°”é€Šæœºåœº", "Pearson": "çš®å°”é€Šæœºåœº", "æœºåœº": "çš®å°”é€Šæœºåœº"
}
if CACHE_PATH.exists():
    with open(CACHE_PATH, "r", encoding="utf-8") as f:
        area_cache = json.load(f)
else:
    area_cache = {}

def normalize_area(text, biz_type="æœªçŸ¥", log_prefix=""):
    """æ ‡å‡†åŒ–å•ä¸ªåœ°ç‚¹å­—ç¬¦ä¸²ï¼Œè‡ªåŠ¨æ—¥å¿—è¾“å‡º"""
    orig_text = text.strip()
    # å…ˆæŸ¥æ˜ å°„
    for k, v in AREA_MAP.items():
        if k.lower() in orig_text.lower():
            norm = v
            break
    else:
        # æŸ¥ç¼“å­˜
        if orig_text in area_cache:
            norm = area_cache[orig_text]
        else:
            # LLMå½’ä¸€
            try:
                prompt = f"è¯·å°†ä¸‹è¿°åœ°å€å½’ä¸€åŒ–ä¸ºGTAåœ°åŒºå¸¸è§çš„è¡Œæ”¿åŒºåï¼ˆå¦‚å¤šä¼¦å¤šã€åŒ—çº¦å…‹ã€å£«å˜‰å ¡ã€ä¸‡é”¦ã€åˆ—æ²»æ–‡å±±ã€å¯†è¥¿æ²™åŠ ã€çš®å°”é€Šæœºåœºç­‰ï¼‰ï¼Œä»…è¿”å›åœ°åï¼Œä¸åŠ å…¶å®ƒï¼š\n{orig_text}"
                resp = openai.chat.completions.create(
                    model=MODEL,
                    messages=[{"role": "system", "content": prompt}],
                    temperature=0, max_tokens=8)
                norm = resp.choices[0].message.content.strip().replace("ã€‚", "")
                norm = norm if norm else orig_text
                area_cache[orig_text] = norm
                with open(CACHE_PATH, "w", encoding="utf-8") as f:
                    json.dump(area_cache, f, ensure_ascii=False, indent=2)
                logging.info(f"{log_prefix}LLMå½’ä¸€: åŸå§‹:ã€Œ{orig_text}ã€â†’ å½’ä¸€:ã€Œ{norm}ã€| ä¸šåŠ¡ç±»å‹: {biz_type}")
            except Exception as e:
                logging.error(f"{log_prefix}LLMå½’ä¸€å¤±è´¥: {e}")
                norm = orig_text
    if not log_prefix.startswith("[Flow]"):  # é¿å…æµå‘æ—¥å¿—å¤ªå¤š
        logging.info(f"{log_prefix}åŒºåŸŸå½’ä¸€: åŸå§‹:ã€Œ{orig_text}ã€â†’ å½’ä¸€:ã€Œ{norm}ã€| ä¸šåŠ¡ç±»å‹: {biz_type}")
    return norm

# ====== ç»“æ„åŒ–â€œè®¢å•æ¦‚è¿°â€ä¸»è¦å­—æ®µï¼ˆåŸæ­£åˆ™+LLMè¡¥é½ï¼‰======
def extract_info(text):
    # ä¸šåŠ¡ç±»å‹
    if re.search(r"æ¥æœº|é€æœº", text):
        type_ = "æ¥é€æœº"
    elif re.search(r"åŒ…è½¦|ä¸€æ—¥æ¸¸|å¤šæ—¥æ¸¸|åŒ…æ—¥", text):
        type_ = "åŒ…è½¦"
    elif re.search(r"è·‘è…¿|ä»£ä¹°|ä»£å–|ä»£é€|ä»£è´­", text):
        type_ = "è·‘è…¿"
    elif re.search(r"è¡Œæå¯„å­˜|å¯„å­˜", text):
        type_ = "è¡Œæå¯„å­˜"
    elif re.search(r"æ¬å®¶|æ¬è¿", text):
        type_ = "æ¬å®¶"
    elif re.search(r"ç”µè¯|å«é†’|å«äºº", text):
        type_ = "ä»£åŠ/å…¶å®ƒ"
    else:
        type_ = ""
    # åŒºåŸŸåˆ¤å®šï¼ˆå…ˆç²—å–ï¼‰
    area_match = re.search(
        r"å¤šä¼¦å¤š|Toronto|çš®å°”é€Š|Markham|Richmond Hill|ä¸‡é”¦|Scarborough|å£«å˜‰å ¡|çº¦å…‹|North York|Etobicoke|å¯†è¥¿æ²™åŠ |Mississauga|æœºåœº",
        text, re.I)
    area_ = area_match.group(0) if area_match else ""
    # é‡‘é¢
    amount_match = re.search(r"[ğŸ’°\$](\d+(\.\d+)?)", text)
    amount = amount_match.group(1) if amount_match else ""
    # èµ·ç‚¹ç»ˆç‚¹
    addresses = re.findall(
        r"ä»\s*([\u4e00-\u9fa5a-zA-Z0-9 ,#\-]+?)(?:åˆ°|â€”|-|ï¼|â€”â€”)\s*([\u4e00-\u9fa5a-zA-Z0-9 ,#\-]+)",
        text)
    if addresses:
        start, end = addresses[0]
    else:
        start, end = "", ""
    # æ—¶é—´
    time_match = re.search(
        r"(\d{1,2}[:ï¼š]\d{2}\s*(?:AM|PM|am|pm)?)|(\d{1,2}ç‚¹åŠ?)|(\d{1,2}/\d{1,2}\s*\d{1,2}[:ï¼š]\d{2})|(?:ä¸Šåˆ|ä¸‹åˆ|ä¸­åˆ)\s*\d{1,2}[:ï¼š]?\d{0,2}",
        text)
    time_ = time_match.group(0) if time_match else ""
    return {
        "ä¸šåŠ¡ç±»å‹_struct": type_,
        "åŒºåŸŸ_struct": area_,
        "é‡‘é¢_struct": amount,
        "èµ·ç‚¹": start,
        "ç»ˆç‚¹": end,
        "æ—¶é—´_struct": time_
    }

extract_results = df["è®¢å•æ¦‚è¿°"].apply(extract_info).apply(pd.Series)
to_llm_idx = extract_results[(extract_results["ä¸šåŠ¡ç±»å‹_struct"] == "") | (extract_results["åŒºåŸŸ_struct"] == "")].index
llm_targets = df.loc[to_llm_idx, "è®¢å•æ¦‚è¿°"].tolist()

def llm_extract(batch):
    out = []
    for text in batch:
        logging.info(f"[LLM-Extract] åŸæ–‡: {text}")
        prompt = (
            "ä½ æ˜¯ä¸šåŠ¡å½’ç±»åŠ©æ‰‹ï¼Œè¯·ä»…è¾“å‡ºå¦‚ä¸‹æ ¼å¼ï¼š\n"
            "ä¸šåŠ¡ç±»å‹: <ç±»å‹>\nåŒºåŸŸ: <åŒºåŸŸ>\n"
            "åªå…è®¸è¿”å›ä¸¤è¡Œï¼Œä¸åŠ å…¶å®ƒæ–‡å­—ã€‚"
        )
        try:
            resp = openai.chat.completions.create(
                model=MODEL,
                messages=[
                    {"role": "system", "content": prompt},
                    {"role": "user", "content": text},
                ],
                temperature=0.0, max_tokens=30,
            )
            ans = resp.choices[0].message.content.strip()
            t = re.search(r"ä¸šåŠ¡ç±»å‹[:ï¼š]\s*(\S+)", ans)
            a = re.search(r"åŒºåŸŸ[:ï¼š]\s*(\S+)", ans)
            biz = t.group(1) if t else ""
            area = a.group(1) if a else ""
            logging.info(f"[LLM-Extract] å½’ä¸€ç»“æœ: ä¸šåŠ¡ç±»å‹: {biz}, åŒºåŸŸ: {area}")
            out.append({
                "ä¸šåŠ¡ç±»å‹_struct": biz,
                "åŒºåŸŸ_struct": area
            })
        except Exception as e:
            logging.error(f"[LLM-Extract] call failed: {e}")
            out.append({"ä¸šåŠ¡ç±»å‹_struct": "", "åŒºåŸŸ_struct": ""})
    return pd.DataFrame(out)

if llm_targets:
    batches = [llm_targets[i:i+10] for i in range(0, len(llm_targets), 10)]
    llm_out = pd.concat([llm_extract(b) for b in batches], ignore_index=True)
    extract_results.loc[to_llm_idx, ["ä¸šåŠ¡ç±»å‹_struct", "åŒºåŸŸ_struct"]] = llm_out.values

# åˆå¹¶å›df
for col in extract_results.columns:
    df[col] = extract_results[col]

# ===== åŒºåŸŸå½’ä¸€ï¼šåº”ç”¨äºæ‰€æœ‰ç›¸å…³å­—æ®µ =====
df["åŒºåŸŸå½’ä¸€"] = [
    normalize_area(area, biz_type=biz, log_prefix="[åŒºåŸŸåˆ†å¸ƒ] ")
    for area, biz in zip(df["åŒºåŸŸ_struct"], df["ä¸šåŠ¡ç±»å‹_struct"])
]

# ====== èµ·ç‚¹/ç»ˆç‚¹å½’ä¸€ï¼ˆæµå‘ç”¨ï¼‰======
df["èµ·ç‚¹å½’ä¸€"] = [
    normalize_area(addr, biz_type=biz, log_prefix="[Flow-èµ·ç‚¹] ")
    for addr, biz in zip(df["èµ·ç‚¹"], df["ä¸šåŠ¡ç±»å‹_struct"])
]
df["ç»ˆç‚¹å½’ä¸€"] = [
    normalize_area(addr, biz_type=biz, log_prefix="[Flow-ç»ˆç‚¹] ")
    for addr, biz in zip(df["ç»ˆç‚¹"], df["ä¸šåŠ¡ç±»å‹_struct"])
]

# ========== å­—æ®µèåˆ ==========

df["ä¸šåŠ¡ç±»å‹_ç»“æ„åŒ–"] = df["ä¸šåŠ¡ç±»å‹_struct"]
if "è®¢å•ç±»å‹" in df.columns:
    df.loc[df["ä¸šåŠ¡ç±»å‹_ç»“æ„åŒ–"] == "", "ä¸šåŠ¡ç±»å‹_ç»“æ„åŒ–"] = df["è®¢å•ç±»å‹"]
df["ä¸šåŠ¡ç±»å‹_å¤§ç±»"] = df.get("è®¢å•ç±»å‹", "")

# ========== ç»Ÿè®¡åˆ†æSheetç”Ÿæˆ ==========

def try_to_float(x):
    try:
        return float(x)
    except:
        return None

report_tables = {}

# 1. ä¸šåŠ¡ç±»å‹åˆ†å¸ƒ_ç»†åˆ†
vc = df["ä¸šåŠ¡ç±»å‹_ç»“æ„åŒ–"].value_counts(dropna=False).reset_index()
vc.columns = ["ä¸šåŠ¡ç±»å‹_ç»†åˆ†", "æ•°é‡"]
report_tables["ä¸šåŠ¡ç±»å‹åˆ†å¸ƒ_ç»†åˆ†"] = vc

# 2. ä¸šåŠ¡ç±»å‹åˆ†å¸ƒ
vc = df["ä¸šåŠ¡ç±»å‹_å¤§ç±»"].value_counts(dropna=False).reset_index()
vc.columns = ["ä¸šåŠ¡ç±»å‹", "æ•°é‡"]
report_tables["ä¸šåŠ¡ç±»å‹åˆ†å¸ƒ"] = vc

# 3. åŒºåŸŸåˆ†å¸ƒï¼ˆå½’ä¸€åŒ–åï¼‰
vc = df["åŒºåŸŸå½’ä¸€"].value_counts(dropna=False).reset_index()
vc.columns = ["åŒºåŸŸ", "æ•°é‡"]
report_tables["åŒºåŸŸåˆ†å¸ƒ"] = vc

# 4. é‡‘é¢åŒºé—´åˆ†å¸ƒ
for col in ["é‡‘é¢_struct", "è®¢å•é‡‘é¢", "é‡‘é¢"]:
    if col in df.columns:
        amount = df[col].apply(try_to_float)
        bins = [0, 50, 100, 200, 500, 1000, float('inf')]
        labels = ["0-50", "50-100", "100-200", "200-500", "500-1000", "1000+"]
        cut = pd.cut(amount, bins=bins, labels=labels, right=False)
        vc = cut.value_counts(sort=False).reset_index()
        vc.columns = ["åŒºé—´", "æ•°é‡"]
        report_tables["é‡‘é¢åŒºé—´åˆ†å¸ƒ"] = vc
        break

# 5. è®¢å•çŠ¶æ€åˆ†å¸ƒ
for col in ["è®¢å•çŠ¶æ€"]:
    if col in df.columns:
        vc = df[col].value_counts(dropna=False).reset_index()
        vc.columns = [col, "æ•°é‡"]
        report_tables["è®¢å•çŠ¶æ€åˆ†å¸ƒ"] = vc
        break

# 6. è¯„åˆ†åŒºé—´åˆ†å¸ƒ
for col in ["è¯„åˆ†", "å®¢æˆ·è¯„åˆ†"]:
    if col in df.columns:
        score = df[col].apply(try_to_float)
        bins = [0, 3, 5, 8, 10]
        labels = ["0-3", "3-5", "5-8", "8-10"]
        cut = pd.cut(score, bins=bins, labels=labels, right=False, include_lowest=True)
        vc = cut.value_counts(sort=False).reset_index()
        vc.columns = ["åŒºé—´", "æ•°é‡"]
        report_tables["è¯„åˆ†åŒºé—´åˆ†å¸ƒ"] = vc
        break

# 7. è¿Ÿåˆ°åˆ†å¸ƒ
for col in ["æ˜¯å¦è¿Ÿåˆ°", "è¿Ÿåˆ°", "è¿Ÿåˆ°æ¬¡æ•°"]:
    if col in df.columns:
        vc = df[col].value_counts(dropna=False).reset_index()
        vc.columns = [col, "æ•°é‡"]
        report_tables["è¿Ÿåˆ°åˆ†å¸ƒ"] = vc
        break

# 8. æŒ‰æœˆè¶‹åŠ¿
for col in ["ä¸‹å•æ—¶é—´", "è®¢å•æ—¥æœŸ", "åˆ›å»ºæ—¶é—´"]:
    if col in df.columns:
        dates = pd.to_datetime(df[col], errors="coerce")
        monthly = dates.dt.to_period("M").value_counts().sort_index().reset_index()
        monthly.columns = ["æœˆä»½", "è®¢å•æ•°"]
        report_tables["æ¯æœˆè®¢å•è¶‹åŠ¿"] = monthly
        break

# 9. å¸æœºåˆ†å¸ƒ
for col in ["å¸æœº", "å¸æœºå§“å", "å¸æœºID"]:
    if col in df.columns:
        vc = df[col].value_counts(dropna=False).reset_index()
        vc.columns = [col, "æ•°é‡"]
        report_tables["å¸æœºåˆ†å¸ƒ"] = vc
        break

# 10. èµ·ç‚¹ç»ˆç‚¹æµå‘åˆ†æï¼ˆå½’ä¸€åŒ–åï¼‰
flow = df.groupby(["èµ·ç‚¹å½’ä¸€", "ç»ˆç‚¹å½’ä¸€"]).size().reset_index(name="è®¢å•æ•°").sort_values("è®¢å•æ•°", ascending=False)
report_tables["æµå‘ç»Ÿè®¡"] = flow

# 11. ä¸šåŠ¡ç±»å‹å¯¹æ¯” (å§‹ç»ˆç”Ÿæˆï¼Œå¦‚æœæ— å·®å¼‚åˆ™ä¸ºç©ºè¡¨)
if "è®¢å•ç±»å‹" in df.columns:
    diff_mask = (df["è®¢å•ç±»å‹"] != df["ä¸šåŠ¡ç±»å‹_ç»“æ„åŒ–"]) & (df["ä¸šåŠ¡ç±»å‹_ç»“æ„åŒ–"] != "")
    df_diff = df.loc[diff_mask, ["è®¢å•æ¦‚è¿°", "ä¸šåŠ¡ç±»å‹_ç»“æ„åŒ–", "ä¸šåŠ¡ç±»å‹_å¤§ç±»"]].rename(
        columns={"ä¸šåŠ¡ç±»å‹_ç»“æ„åŒ–": "ç»“æ„åŒ–ä¸šåŠ¡ç±»å‹", "ä¸šåŠ¡ç±»å‹_å¤§ç±»": "è®¢å•ç±»å‹"}
    )
else:
    df_diff = pd.DataFrame(columns=["è®¢å•æ¦‚è¿°", "ç»“æ„åŒ–ä¸šåŠ¡ç±»å‹", "è®¢å•ç±»å‹"])
report_tables["ä¸šåŠ¡ç±»å‹å¯¹æ¯”"] = df_diff

# 12. æ˜ç»†å…¨è¡¨
report_tables["æ˜ç»†å…¨è¡¨"] = df

# ===== è¾“å‡ºExcelï¼Œå¤šsheetè‡ªåŠ¨åˆ—å®½å’Œè¡¨æ ¼æ ·å¼ =====
with pd.ExcelWriter(XLSX_OUT, engine="openpyxl", mode="w") as writer:
    for name, table in report_tables.items():
        table.to_excel(writer, index=False, sheet_name=name[:31])

def auto_adjust_column_width_and_style(xlsx_path: Path):
    wb = load_workbook(xlsx_path)
    for ws in wb.worksheets:
        for col in ws.columns:
            max_length = max(len(str(cell.value or "")) for cell in col)
            col_letter = get_column_letter(col[0].column)
            ws.column_dimensions[col_letter].width = max_length + 2
        if ws.title != "æ˜ç»†å…¨è¡¨":
            end_row = ws.max_row
            end_col = ws.max_column
            if end_row > 1 and end_col > 0:
                tab = Table(displayName=f"Table_{ws.title.replace(' ', '_')}",
                            ref=f"A1:{get_column_letter(end_col)}{end_row}")
                style = TableStyleInfo(name="TableStyleMedium9", showFirstColumn=False,
                                      showLastColumn=False, showRowStripes=True, showColumnStripes=False)
                tab.tableStyleInfo = style
                ws.add_table(tab)
    wb.save(xlsx_path)

auto_adjust_column_width_and_style(XLSX_OUT)
logging.info("âœ… æ‰€æœ‰ç»Ÿè®¡å®Œæˆï¼Œåˆ†æExcelå·²è¾“å‡ºåˆ°å¸æœºä¸šåŠ¡_ç»Ÿè®¡åˆ†æ.xlsxï¼")
